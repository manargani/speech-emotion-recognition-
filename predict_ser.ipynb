{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manargani/speech-emotion-recognition-/blob/main/predict_ser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show pyaudio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EEAXPs6qYGL",
        "outputId": "9bd6af67-59fb-4bca-b23b-b264a28d71a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Package(s) not found: pyaudio\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNoZxCQVb6Gd"
      },
      "outputs": [],
      "source": [
        "#import pandas as pd\n",
        "import numpy as np\n",
        "#import os\n",
        "#import sys\n",
        "#import random\n",
        "# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.\n",
        "import librosa\n",
        "#import librosa.display\n",
        "#import seaborn as sns\n",
        "#import matplotlib.pyplot as plt\n",
        "#sns.set_theme(style=\"whitegrid\")\n",
        "#from tqdm import tqdm\n",
        "#from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "#from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "#from sklearn.model_selection import train_test_split\n",
        "# to play the audio files\n",
        "#from IPython.display import Audio\n",
        "#from tensorflow import keras\n",
        "#from tensorflow.keras import layers\n",
        "#from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "#from keras.models import Sequential, model_from_json\n",
        "#from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization, LSTM, Bidirectional\n",
        "#from keras.utils import np_utils\n",
        "#from tensorflow.keras.utils import to_categorical\n",
        "#from keras.callbacks import ModelCheckpoint\n",
        "#import warnings\n",
        "#if not sys.warnoptions:\n",
        "#    warnings.simplefilter(\"ignore\")\n",
        "#warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "#from tensorflow import keras\n",
        "#from tensorflow.keras import layers\n",
        "#from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "#from keras.models import Sequential\n",
        "#from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization, LSTM, Bidirectional\n",
        "#from keras.utils import np_utils, to_categorical\n",
        "#from keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "EwR8sZE7A1HF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path ='/content/drive/MyDrive/Kaggle/reel/3.wav'\n",
        "path"
      ],
      "metadata": {
        "id": "Gs0eWMxWCXFw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "32c52d12-13ea-42d7-87b1-2f83b7132af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Kaggle/reel/3.wav'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def noise(data):\n",
        "    noise_amp = 0.04*np.random.uniform()*np.amax(data)\n",
        "    data = data + noise_amp*np.random.normal(size=data.shape[0])\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "_a3GpFvarQ2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sample_rate = 22050\n",
        "def extract_features(data):\n",
        "\n",
        "    result = np.array([])\n",
        "\n",
        "    mfccs = librosa.feature.mfcc(y=data, sr=22050, n_mfcc=58)\n",
        "    mfccs_processed = np.mean(mfccs.T,axis=0)\n",
        "    result = np.array(mfccs_processed)\n",
        "\n",
        "    return result\n",
        "\n",
        "def get_features(path):\n",
        "    # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.\n",
        "    data, sample_rate = librosa.load(path, duration=3, offset=0.5)\n",
        "\n",
        "    #without augmentation\n",
        "    res1 = extract_features(data)\n",
        "    result = np.array(res1)\n",
        "\n",
        "\n",
        "    #noised\n",
        "    noise_data = noise(data)\n",
        "    res2 = extract_features(noise_data)\n",
        "    result = np.vstack((result, res2))\n",
        "\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "WUzMta2uCPjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = [], []\n",
        "#path = np.array(data_path.path)[0]\n",
        "#for path, emotion in tqdm(zip(data_path.path, data_path.labels)):\n",
        "features = get_features(path)\n",
        "for feature in features:\n",
        "        X.append(feature)\n",
        "        # appending emotion 5 times as we have made 5 augmentation techniques on each audio file.\n",
        "        # Y.append('1')"
      ],
      "metadata": {
        "id": "pyj8nTCaCLvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "i4TPZ-OnpLcW",
        "outputId": "e1d2e919-fdca-43f5-c26b-f2be5a388966",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(X)\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4jesZRGCE58",
        "outputId": "f9005ac4-4756-426e-801e-82cae229f65b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([-6.70195435e+02,  6.50638504e+01,  8.88955355e-01,  1.47159796e+01,\n",
              "         9.18216324e+00,  6.60576046e-01, -3.84683704e+00, -3.58394504e+00,\n",
              "        -1.29590063e+01, -3.30013347e+00,  9.10780847e-01, -3.59703636e+00,\n",
              "         2.37627506e+00, -4.38894129e+00,  5.45079589e-01,  8.91852617e-01,\n",
              "        -4.80258989e+00, -2.10541463e+00, -1.60596776e+00, -1.05239093e+00,\n",
              "        -7.06728077e+00, -6.23060465e-01, -2.72803068e+00, -5.30735970e+00,\n",
              "        -1.96911705e+00, -9.46152687e-01, -5.72111034e+00,  3.32990497e-01,\n",
              "        -2.54384422e+00,  1.82208031e-01, -2.35109782e+00, -2.50472760e+00,\n",
              "        -3.15150547e+00, -2.19089890e+00, -3.80176091e+00, -1.81308663e+00,\n",
              "        -1.26122320e+00, -2.14495444e+00, -4.15217209e+00, -1.77961588e+00,\n",
              "        -4.90007544e+00, -6.85950279e-01, -3.20931625e+00, -2.98257613e+00,\n",
              "        -3.46229291e+00, -2.07138038e+00, -3.75293565e+00, -2.42382929e-01,\n",
              "        -3.33463597e+00, -6.66891038e-01, -1.96219957e+00, -1.45200086e+00,\n",
              "        -2.24830604e+00,  2.04601005e-01, -1.20861912e+00, -5.28246105e-01,\n",
              "        -2.10304761e+00, -1.59894395e+00]),\n",
              " array([-4.82814104e+02,  3.31942174e+01,  6.37793667e+00,  9.25156279e+00,\n",
              "         5.53099807e+00, -6.02667077e-01, -3.33911227e+00, -6.66409773e+00,\n",
              "        -9.30102690e+00, -5.72432236e+00, -1.06251859e+00, -2.00532633e+00,\n",
              "        -2.06654477e+00, -2.07585416e+00, -7.05576836e-01, -9.09170320e-01,\n",
              "        -3.86911320e+00, -2.84264384e+00,  1.26485224e-01, -2.48801950e+00,\n",
              "        -4.53766507e+00, -1.93914936e+00, -2.34790672e+00, -4.44834729e+00,\n",
              "        -1.52106386e+00, -1.37161259e+00, -3.01434002e+00, -7.97660104e-01,\n",
              "        -1.19158791e+00, -9.03503346e-01, -1.45165842e+00, -2.24610583e+00,\n",
              "        -1.96756026e+00, -2.46485703e+00, -2.36283960e+00, -1.85634616e+00,\n",
              "        -1.35446775e+00, -1.83465253e+00, -2.82591295e+00, -2.31588306e+00,\n",
              "        -2.58392783e+00, -1.53944732e+00, -1.69892139e+00, -2.67558706e+00,\n",
              "        -2.74352798e+00, -1.64860039e+00, -1.67667758e+00, -5.41925567e-01,\n",
              "        -1.62321329e+00, -9.31638084e-01, -1.28816789e-01, -5.87492762e-01,\n",
              "        -5.71451690e-01, -3.12221525e-01, -1.28521318e-01, -5.88030047e-01,\n",
              "        -9.52416107e-01, -7.56900236e-01])]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# Charger le transformateur\n",
        "with open('/content/drive/MyDrive/Kaggle/encoder.pkl', 'rb') as f:\n",
        "    encoder = pickle.load(f)\n",
        "\n",
        "\n",
        "\"\"\"encoder = OneHotEncoder()\n",
        "Y_res = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()\n",
        "Y_res\"\"\"\n",
        "encoder"
      ],
      "metadata": {
        "id": "TCdGpky3BkDZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "796e3f86-6cd4-4eff-910b-7056f9b0d10c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OneHotEncoder()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneHotEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#x_train = np.expand_dims(x_train, axis=2)\n",
        "X = np.expand_dims(X, axis=2)\n",
        "#x_train.shape, y_train.shape, x_test.shape, y_test.shape\n",
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bpp-AuFCUv_C",
        "outputId": "102bf5b6-28c6-46f9-dd36-689c19ee52ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 58, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = tf.keras.models.load_model('/content/drive/MyDrive/Kaggle/model/Emotion_Model_conv1d_gender_93.h5')"
      ],
      "metadata": {
        "id": "pOL-6Cy6tIpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = loaded_model.predict(X)\n",
        "preds.shape"
      ],
      "metadata": {
        "id": "XywojHn3LrHz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "878a5d6a-eca2-44c1-e7c8-6406b1a8074a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# predicting on test data.\n",
        "y_pred = encoder.inverse_transform(preds)\n",
        "#y_tmp = encoder.inverse_transform(y_test)"
      ],
      "metadata": {
        "id": "2slnUFWlLuTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_pred)"
      ],
      "metadata": {
        "id": "jqThz0SCPZu0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecce8549-6bfd-41b3-9973-bbe039a42e8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['female_happy']\n",
            " ['female_happy']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Lab=y_pred[0][0]\n",
        "Lab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "GtU7DOi9VK2Z",
        "outputId": "2df3708e-a22b-4dd6-a729-398edb963d00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'female_happy'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if Lab == 'female_angry' or Lab == 'male_angry':\n",
        "    ActualLabel = 'angry'\n",
        "elif Lab == 'female_disgust' or Lab == 'male_disgust':\n",
        "    ActualLabel = 'disgust'\n",
        "elif Lab == 'female_fear' or Lab == 'male_fear':\n",
        "    ActualLabel = 'fear'\n",
        "elif Lab == 'female_happy' or Lab == 'male_happy':\n",
        "    ActualLabel = 'disgust'\n",
        "elif Lab == 'female_sad' or Lab == 'male_sad':\n",
        "    ActualLabel = 'sad'\n",
        "elif Lab == 'female_surprise' or Lab == 'male_surprise':\n",
        "    ActualLabel = 'surprise'\n",
        "elif Lab == 'female_neutral' or Lab == 'male_neutral':\n",
        "    ActualLabel = 'neutral'\n",
        "else:\n",
        "    ActualLabel = 'Unknown'"
      ],
      "metadata": {
        "id": "X1FgypRyVnxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print( ActualLabel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVW3RTQyZcSO",
        "outputId": "483788df-dcec-4f21-d323-eaabf50ebdd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neutral\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_resampled = librosa.resample(data, sr_orig, sr_target)"
      ],
      "metadata": {
        "id": "vS5FHTjFGKOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "\n",
        "# Record a short audio clip using your microphone and save it as a WAV file\n",
        "# Replace \"input.wav\" with the path and filename of your WAV file\n",
        "# Be sure to record a long enough clip to accurately determine the sample rate\n",
        "# For example, record at least 5 seconds of audio\n",
        "# You can use a tool like Audacity to record and save the audio clip\n",
        "input_file = \"/content/drive/MyDrive/Kaggle/reel/3.wav\"\n",
        "\n",
        "# Load the WAV file into Python using librosa\n",
        "# The sample rate will be returned in the \"sr\" variable\n",
        "data, sr = librosa.load(input_file, sr=None, mono=True)\n",
        "\n",
        "# Print the sample rate of the loaded audio clip\n",
        "print(\"Sample rate of microphone:\", sr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sy_jaUXuGhAj",
        "outputId": "9273ce28-ea72-4f9a-b077-8b17747080b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample rate of microphone: 22050\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pyaudio\n",
        "import tensorflow as tf\n",
        "import Adafruit_CharLCD as LCD\n",
        "\n",
        "# Load pre-trained model\n",
        "model = tf.keras.models.load_model('path/to/model.h5')\n",
        "\n",
        "# Initialize Pyaudio stream\n",
        "CHUNK = 1024\n",
        "FORMAT = pyaudio.paInt16\n",
        "CHANNELS = 1\n",
        "RATE = 16000\n",
        "p = pyaudio.PyAudio()\n",
        "stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK)\n",
        "\n",
        "# Initialize LCD screen\n",
        "lcd = LCD.Adafruit_CharLCDPlate()\n",
        "\n",
        "# Loop to stream audio and predict emotions every 3 seconds\n",
        "start_time = time.time()\n",
        "counter = 0\n",
        "while True:\n",
        "    data = np.fromstring(stream.read(CHUNK), dtype=np.int16)\n",
        "    data = np.divide(data, 32768.0)  # Normalize the data\n",
        "    emotion = model.predict(np.array([data]))[0]\n",
        "    emotion_label = np.argmax(emotion)\n",
        "    counter += 1\n",
        "    if counter == int(RATE / CHUNK * 3):  # Predict emotion every 3 seconds\n",
        "        if emotion_label == 0:\n",
        "            lcd.clear()\n",
        "            lcd.message('Emotion:\\nNeutral')\n",
        "        elif emotion_label == 1:\n",
        "            lcd.clear()\n",
        "            lcd.message('Emotion:\\nCalm')\n",
        "        elif emotion_label == 2:\n",
        "            lcd.clear()\n",
        "            lcd.message('Emotion:\\nHappy')\n",
        "        elif emotion_label == 3:\n",
        "            lcd.clear()\n",
        "            lcd.message('Emotion:\\nSad')\n",
        "        elif emotion_label == 4:\n",
        "            lcd.clear()\n",
        "            lcd.message('Emotion:\\nAngry')\n",
        "        elif emotion_label == 5:\n",
        "            lcd.clear()\n",
        "            lcd.message('Emotion:\\nFearful')\n",
        "        elif emotion_label == 6:\n",
        "            lcd.clear()\n",
        "            lcd.message('Emotion:\\nDisgusted')\n",
        "        elif emotion_label == 7:\n",
        "            lcd.clear()\n",
        "            lcd.message('Emotion:\\nSurprised')\n",
        "        counter = 0  # Reset counter after predicting emotion\n",
        "        start_time = time.time()  # Reset start time\n",
        "    if time.time() - start_time >= 3:  # Reset start time if it takes more than 3 seconds to read a chunk of audio data\n",
        "        start_time = time.time()"
      ],
      "metadata": {
        "id": "PD8PlLofMaNq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}